[0m[[0m[0mdebug[0m] [0m[0mjavaOptions: Vector(--add-exports=java.base/sun.nio.ch=ALL-UNNAMED, -Xms512M, -Xmx2048M)[0m
[0m[[0m[0mdebug[0m] [0m[0mForking tests - parallelism = false[0m
[0m[[0m[0mdebug[0m] [0m[0mCreate a single-thread test executor[0m
[0m[[0m[0mdebug[0m] [0m[0mRunner for org.scalatest.tools.Framework produced 4 initial tasks for 4 tests.[0m
[0m[[0m[0mdebug[0m] [0m[0m  Running TaskDef(com.example.service.StreamProcessingSpec, sbt.ForkMain$SubclassFingerscan@2d928643, false, [SuiteSelector])[0m
[0m[[0m[0minfo[0m] [0m[0m[32mStreamProcessingSpec:[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[32mStream processor[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m- should handle basic stream operations *** FAILED ***[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.AnalysisException: This query does not support recovering from checkpoint location. Delete target/checkpoint/test_stream/offsets to start over.[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.errors.QueryCompilationErrors$.recoverQueryFromCheckpointUnsupportedError(QueryCompilationErrors.scala:2828)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:95)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:42)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:40)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  ...[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m- should handle error scenarios in stream *** FAILED ***[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.AnalysisException: This query does not support recovering from checkpoint location. Delete target/checkpoint/error_stream/offsets to start over.[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.errors.QueryCompilationErrors$.recoverQueryFromCheckpointUnsupportedError(QueryCompilationErrors.scala:2828)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:95)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:42)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:40)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  ...[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m- should maintain state across batches *** FAILED ***[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.AnalysisException: Event time must be defined on a window or a timestamp, but event_time is of type string.;[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31mEventTimeWatermark event_time#47: string, 10 minutes[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m+- Project [data#43.id AS id#45, data#43.value AS value#46, data#43.event_time AS event_time#47][0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m   +- Project [from_json(StructField(id,StringType,true), StructField(value,DoubleType,true), StructField(event_time,StringType,true), value#41, Some(Europe/Riga)) AS data#43][0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m      +- StreamingRelationV2 org.apache.spark.sql.execution.streaming.MemoryStreamTableProvider$@65d1110e, memory, org.apache.spark.sql.execution.streaming.MemoryStreamTable@26984897, [], [value#41][0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:52)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1(CheckAnalysis.scala:367)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$1$adapted(CheckAnalysis.scala:163)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:295)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:163)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:160)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:188)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:156)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:146)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:188)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  ...[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    Produced 0 nested tasks and 3 events.[0m
[0m[[0m[0mdebug[0m] [0m[0m  Running TaskDef(com.example.service.JsonParsingSpec, sbt.ForkMain$SubclassFingerscan@5025a98f, false, [SuiteSelector])[0m
[0m[[0m[0minfo[0m] [0m[0m[32mJsonParsingSpec:[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[32mJSON Parser[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[32m- should handle all valid data type combinations[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m- should handle missing required fields *** FAILED ***[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  2 was not equal to 0 (JsonParsingSpec.scala:60)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m- should handle invalid data types *** FAILED ***[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  2 was not equal to 0 (JsonParsingSpec.scala:75)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[32m- should handle malformed JSON strings[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m- should handle empty and null values *** FAILED ***[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  1 was not equal to 0 (JsonParsingSpec.scala:107)[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    Produced 0 nested tasks and 5 events.[0m
[0m[[0m[0mdebug[0m] [0m[0m  Running TaskDef(com.example.service.KafkaProcessingSpec, sbt.ForkMain$SubclassFingerscan@49993335, false, [SuiteSelector])[0m
[0m[[0m[0minfo[0m] [0m[0m[32mKafkaProcessingSpec:[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[32mKafka message processor[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[32m- should convert binary messages to strings[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[32m- should handle empty messages[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[32m- should handle oversized messages[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    Produced 0 nested tasks and 3 events.[0m
[0m[[0m[0mdebug[0m] [0m[0m  Running TaskDef(ScalaServiceSpec, sbt.ForkMain$SubclassFingerscan@20322d26, false, [SuiteSelector])[0m
[0m[[0m[0minfo[0m] [0m[0m[32mScalaServiceSpec:[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[32mJSON message parsing[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[32m- should correctly extract fields from valid JSON[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[32mKafka message processing[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[32m- should convert messages to the correct format[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[32mThe full pipeline[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m- should process messages end-to-end *** FAILED ***[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  org.apache.spark.sql.AnalysisException: This query does not support recovering from checkpoint location. Delete target/checkpoint/results/offsets to start over.[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.errors.QueryCompilationErrors$.recoverQueryFromCheckpointUnsupportedError(QueryCompilationErrors.scala:2828)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$.resolveCheckpointLocation(ResolveWriteToStream.scala:95)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:42)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.execution.streaming.ResolveWriteToStream$$anonfun$apply$1.applyOrElse(ResolveWriteToStream.scala:40)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$2(AnalysisHelper.scala:170)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsDownWithPruning$1(AnalysisHelper.scala:170)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:323)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning(AnalysisHelper.scala:168)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsDownWithPruning$(AnalysisHelper.scala:164)[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m  ...[0m[0m
[0m[[0m[0mdebug[0m] [0m[0m    Produced 0 nested tasks and 3 events.[0m
[0m[[0m[0minfo[0m] [0m[0m[36mRun completed in 6 seconds, 955 milliseconds.[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[36mTotal number of tests run: 14[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[36mSuites: completed 4, aborted 0[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[36mTests: succeeded 7, failed 7, canceled 0, ignored 0, pending 0[0m[0m
[0m[[0m[0minfo[0m] [0m[0m[31m*** 7 TESTS FAILED ***[0m[0m
[0m[[0m[0mdebug[0m] [0m[0mPassed tests:[0m
[0m[[0m[0mdebug[0m] [0m[0m	com.example.service.KafkaProcessingSpec[0m
[0m[[0m[31merror[0m] [0m[0mFailed tests:[0m
[0m[[0m[31merror[0m] [0m[0m	ScalaServiceSpec[0m
[0m[[0m[31merror[0m] [0m[0m	com.example.service.StreamProcessingSpec[0m
[0m[[0m[31merror[0m] [0m[0m	com.example.service.JsonParsingSpec[0m
[0m[[0m[31merror[0m] [0m[0m(Test / [31mtestOnly[0m) sbt.TestsFailedException: Tests unsuccessful[0m
